{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Ang Mink Chen\n",
    "###### Python version: Python 3\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd # Just to tabulate the confusion matrix so that it's easier to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens a data file in csv, and transforms it into a usable format (2D array)\n",
    "# Assumptions for this implementation: \n",
    "# 1. Data in the datasets provided are always valid: \n",
    "#    a) All instances have same amount of attributes. \n",
    "#    b) Data are recorded properly (no human errors/typos).\n",
    "#    c) Missing values are due to exterior factors beyond the scope of this project.\n",
    "\n",
    "def preprocess(filename):\n",
    "    f = open(filename, 'r')\n",
    "    data = []\n",
    "    target = []\n",
    "    for line in f.readlines():\n",
    "        instance = line.strip().split(',')\n",
    "        data.append(instance[:-1])\n",
    "        target.append(instance[-1])\n",
    "    f.close()    \n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function builds a supervised NB model\n",
    "def train(data, target):\n",
    "    \n",
    "    model = {}\n",
    "    \n",
    "    # Iterate through each instance\n",
    "    for ctr in range(len(data)):\n",
    "        \n",
    "        ### Count labels occurrences ### \n",
    "        label = target[ctr]\n",
    "        # Build up the dict structure procedurally for labels\n",
    "        if label not in model.keys(): model[label] = {}\n",
    "        # Initialize the count to 1 if it has not been encountered before or increment the count by 1\n",
    "        model[label]['count'] = [lambda:1, lambda:model[label]['count']+1][bool(model[label])]()\n",
    "        \n",
    "        ### Count attributes occurrences ###\n",
    "        attributes = data[ctr]\n",
    "        # Iterate through each data attribute\n",
    "        for i in range(len(attributes)):\n",
    "            attr = attributes[i]\n",
    "            # Build up the dict structure procedurally for attributes \n",
    "            # Attributes are named after their indexes in the dataset (e.g. attribute at index 0 is named 0)\n",
    "            if i not in model[label].keys(): model[label][i] = {}\n",
    "            # Initialize the count to 1 if it has not been encountered before or increment the count by 1\n",
    "            model[label][i][attr] = [lambda:1, lambda:model[label][i][attr]+1][attr in model[label][i].keys()]()\n",
    "    \n",
    "    \n",
    "    ### Make sure each attribute contains all possible attribute values ###\n",
    "    # Iterate through the model created to find all possible values for each attribute\n",
    "    attr_values_dict = {}    \n",
    "    for label in model.keys():\n",
    "        for attr in model[label].keys():\n",
    "            if attr == 'count': continue\n",
    "            if attr not in attr_values_dict.keys(): \n",
    "                attr_values_dict[attr] = []\n",
    "            for attr_val in model[label][attr].keys():\n",
    "                if attr_val not in attr_values_dict[attr]: \n",
    "                    attr_values_dict[attr].append(attr_val)\n",
    "    \n",
    "    # Iterate through the model, check if all values of each attribute has been recorded\n",
    "    # (especially for a value with count 0 as it will not be recorded in the model initially), \n",
    "    # and initialize all unrecorded attributes values with count 0\n",
    "    for label in model.keys():\n",
    "        for attr in model[label].keys():\n",
    "            if attr == 'count': continue\n",
    "            for attr_val in attr_values_dict[attr]:\n",
    "                if attr_val not in model[label][attr].keys():\n",
    "                    model[label][attr][attr_val] = 0\n",
    "                    \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the class for an instance or a set of instances based on a trained NB model \n",
    "def predict(nb_model, instances, k=1):\n",
    "    \n",
    "    # Array to store all the predictions\n",
    "    predictions = []\n",
    "        \n",
    "    for instance in instances:\n",
    "        \n",
    "        # Dict structure to store all the class probabilities calculated\n",
    "        p = {}\n",
    "        \n",
    "        ### Get total count of the instances ###\n",
    "        total_count = 0\n",
    "        for label in nb_model.keys():\n",
    "            total_count += nb_model[label]['count']\n",
    "\n",
    "        ### Calculate the probability for each label prediction ###\n",
    "        # Iterate through each label\n",
    "        for label in nb_model.keys():\n",
    "            \n",
    "            posterior_prob = 1\n",
    "            # Iterate through each attribute to calulate posterior probability\n",
    "            for i in range(len(instance)):\n",
    "                attr = instance[i]\n",
    "                # Ignore missing values\n",
    "                if attr == '?': continue\n",
    "                # Calculate the probability based on counts recorded in the model    \n",
    "                posterior_prob *= nb_model[label][i][attr]/nb_model[label]['count']\n",
    "            \n",
    "            # Recalculate posterior probability with add-k smoothing if there is a previously unseen test \n",
    "            # instance that caused the posterior probability to be 0\n",
    "            if posterior_prob == 0:\n",
    "                posterior_prob = 1\n",
    "                # Iterate through each attribute to calulate posterior probability \n",
    "                for i in range(len(instance)):\n",
    "                    attr = instance[i]\n",
    "                    # Ignore missing values\n",
    "                    if attr == '?': continue\n",
    "                    # Calculate the probability with add-k smoothing  \n",
    "                    posterior_prob *= (nb_model[label][i][attr]+k)/(nb_model[label]['count']+(len(nb_model[label][i].keys())*k))\n",
    "            \n",
    "            # Calculate the prior probability    \n",
    "            prior_prob = nb_model[label]['count']/(total_count+0.0)\n",
    "            \n",
    "            # Calculate the label probability using prior and posterior probability, then store it \n",
    "            p[label] = prior_prob * posterior_prob\n",
    "\n",
    "        ### Get the predicted label with highest probability ###\n",
    "        prediction = None\n",
    "        highest_prob = -1\n",
    "        for label, prob in p.items():\n",
    "            if prob > highest_prob:\n",
    "                highest_prob = prob\n",
    "                prediction = label\n",
    "        \n",
    "        # Store the predicted label\n",
    "        predictions.append(prediction)        \n",
    "                \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates a set of predictions, in a supervised context \n",
    "def evaluate(predictions, target):\n",
    "    \n",
    "    # Get all the labels\n",
    "    labels = []\n",
    "    for t in target:\n",
    "        if t not in labels:\n",
    "            labels.append(t)\n",
    "    \n",
    "    # Initialize confusion matrix as dict with the required evaluation metrics\n",
    "    con_matrix = {}\n",
    "    con_matrix['Precision'] = []\n",
    "    con_matrix['Recall']    = []\n",
    "    con_matrix['Accuracy']  = []\n",
    "    con_matrix['F1_Score']  = []\n",
    "    \n",
    "    # Iterate through each class to calculate the classifier's performance (evaluation metrics) \n",
    "    # with respect to the class\n",
    "    for label in labels:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        # Iterate through all test instances\n",
    "        for i in range(len(predictions)):\n",
    "            pred = predictions[i]\n",
    "            \n",
    "            # Set the selected label as positive (interested class)\n",
    "            if pred == label:\n",
    "                # Predict positive class and the target is positive class\n",
    "                if pred == target[i]:\n",
    "                    tp += 1\n",
    "                # Predict positive class but the target is negative class\n",
    "                else:   \n",
    "                    fp += 1\n",
    "                    \n",
    "            # Set all other labels as negative (uninterested class)     \n",
    "            else:\n",
    "                # Predict negative class but the target is positive class\n",
    "                if target[i] == label:\n",
    "                    fn += 1\n",
    "                # Predict negative class and the target is negative class    \n",
    "                else:\n",
    "                    tn += 1\n",
    "        \n",
    "        # Perform calculations and store the results for all required evaluation metrics\n",
    "        precision = [lambda: tp / (tp + fp), lambda: 0][tp + fp == 0]()\n",
    "        recall    = [lambda: tp / (tp + fn), lambda: 0][tp + fn == 0]()\n",
    "        accuracy  = [lambda: (tp + tn) / (tp + fp + tn + fn), lambda: 0][tp + fp + tn + fn == 0]()\n",
    "        f1_score  = [lambda: 2 * (precision * recall) / (precision + recall), lambda: 0][(precision + recall) == 0]()\n",
    "        \n",
    "        con_matrix['Precision'].append(precision)\n",
    "        con_matrix['Recall'].append(recall)\n",
    "        con_matrix['Accuracy'].append(accuracy)\n",
    "        con_matrix['F1_Score'].append(f1_score)\n",
    "    \n",
    "    ### Create DataFrame to display the confusion matrix ###\n",
    "    cm_table = pd.DataFrame()\n",
    "    cm_table['Precision'] = con_matrix['Precision']\n",
    "    cm_table['Recall']    = con_matrix['Recall']\n",
    "    cm_table['Accuracy']  = con_matrix['Accuracy']\n",
    "    cm_table['F1_Score']  = con_matrix['F1_Score']\n",
    "    \n",
    "    cm_table.index = labels\n",
    "    \n",
    "    # Calculate the macro-averaged for each evaluation metric\n",
    "    row = pd.Series({'Precision':sum(con_matrix['Precision'])/len(con_matrix['Precision']),\n",
    "                     'Recall'   :sum(con_matrix['Recall'])/len(con_matrix['Recall']),\n",
    "                     'Accuracy' :sum(con_matrix['Accuracy'])/len(con_matrix['Accuracy']),\n",
    "                     'F1_Score' :sum(con_matrix['F1_Score'])/len(con_matrix['F1_Score'])},\n",
    "                     name='macro-averaged')\n",
    "    cm_table = cm_table.append(row)\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(cm_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(nb_model, attr_indexes):\n",
    "    \n",
    "    # Dict to store the information gain calculated of each attribute  \n",
    "    igs = {}\n",
    "    \n",
    "    # First get all the classes\n",
    "    classes = nb_model.keys()\n",
    "    # Get total count of the instances\n",
    "    total_count = 0\n",
    "    for label in nb_model.keys():\n",
    "        total_count += nb_model[label]['count']\n",
    "\n",
    "    ### Calculate the entropy at the root node ###\n",
    "    h_r = 0\n",
    "    for c in classes:\n",
    "        prob = nb_model[c]['count']/(total_count+0.0)\n",
    "        h_r += prob*math.log(prob, 2)\n",
    "    h_r *= -1\n",
    "    \n",
    "    # Store the entropy at the root node\n",
    "    igs['h_r'] = h_r\n",
    "    \n",
    "    # Iterate through each attribute via its index value\n",
    "    for index in attr_indexes:\n",
    "        \n",
    "        ### Calculate mean information for the selected attribute ###\n",
    "        # First extract the count of the selected attribute (preprocessing)\n",
    "        # store it in a new dict structure for easier retrieval\n",
    "        p = {}\n",
    "        for c in classes:\n",
    "            for attr_val in nb_model[c][index].keys():\n",
    "                # Build up the dict structure procedurally with attribute values \n",
    "                if attr_val not in p.keys(): p[attr_val] = {}\n",
    "                p[attr_val][c] = nb_model[c][index][attr_val]\n",
    "                \n",
    "        # Actual calculation of mean information here\n",
    "        mean_info = 0\n",
    "        for attr_val in p.keys():\n",
    "            # Count the number of instances that have the attribute value\n",
    "            count = 0\n",
    "            for label in p[attr_val].keys(): count += p[attr_val][label]\n",
    "            # Calculate the entropy for the selected attribute    \n",
    "            attr_entropy = 0    \n",
    "            for label in p[attr_val].keys():\n",
    "                prob = p[attr_val][label]/(count+0.0)\n",
    "                if prob == 0: continue\n",
    "                attr_entropy += prob*math.log(prob, 2)\n",
    "            attr_entropy *= -1    \n",
    "            mean_info += (count/total_count) * attr_entropy\n",
    "        \n",
    "        ### Calculate the information gain for the selected attribute ###\n",
    "        ig = h_r - mean_info\n",
    "        \n",
    "        # Store the information gain value\n",
    "        igs[index] = ig\n",
    "    \n",
    "    return igs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: anneal.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "3                0.998418  0.922515  0.939866  0.958967\n",
      "U                0.493827  1.000000  0.954343  0.661157\n",
      "1                0.888889  1.000000  0.998886  0.941176\n",
      "5                1.000000  1.000000  1.000000  1.000000\n",
      "2                0.899083  0.989899  0.986637  0.942308\n",
      "macro-averaged   0.856043  0.982483  0.975947  0.900722\n",
      "\n",
      "Root entropy: 1.189833856204398\n",
      "IG at  0: 0.40908953764451006\n",
      "IG at  1: 0.0\n",
      "IG at  2: 0.3060515354289405\n",
      "IG at  3: 0.051344088764404106\n",
      "IG at  4: 0.29108220585994726\n",
      "IG at  5: 0.1471188622809556\n",
      "IG at  6: 0.2137228803159087\n",
      "IG at  7: 0.29223544065798446\n",
      "IG at  8: 0.1261663361036096\n",
      "IG at  9: 0.14107379163812883\n",
      "IG at 10: 0.032488406491841815\n",
      "IG at 11: 0.43517783626288575\n",
      "IG at 12: 0.03870173274881061\n",
      "IG at 13: 0.00043760652021185287\n",
      "IG at 14: 0.03935557414283708\n",
      "IG at 15: 0.021775078259213876\n",
      "IG at 16: 0.037997478813511565\n",
      "IG at 17: 0.03670308136440825\n",
      "IG at 18: 0.0\n",
      "IG at 19: 0.11722522630372034\n",
      "IG at 20: 0.029753745208638938\n",
      "IG at 21: 0.02704235332867677\n",
      "IG at 22: 0.0\n",
      "IG at 23: 0.015604780443500665\n",
      "IG at 24: 0.13718113252042574\n",
      "IG at 25: 0.0\n",
      "IG at 26: 0.0223970898516459\n",
      "IG at 27: 0.01824168402125048\n",
      "IG at 28: 0.0\n",
      "IG at 29: 0.0\n",
      "IG at 30: 0.0\n",
      "IG at 31: 0.04323960556514961\n",
      "IG at 32: 0.03303757117705719\n",
      "IG at 33: 0.01937886432831948\n",
      "IG at 34: 0.003958783545891853\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: breast-cancer.csv\n",
      "\n",
      "                      Precision    Recall  Accuracy  F1_Score\n",
      "recurrence-events      0.597403  0.541176  0.755245  0.567901\n",
      "no-recurrence-events   0.813397  0.845771  0.755245  0.829268\n",
      "macro-averaged         0.705400  0.693474  0.755245  0.698585\n",
      "\n",
      "Root entropy: 0.8778446951746506\n",
      "IG at  0: 0.010605956535614136\n",
      "IG at  1: 0.0020016149737116518\n",
      "IG at  2: 0.05717112532429669\n",
      "IG at  3: 0.06899508808988597\n",
      "IG at  4: 0.05342264225173787\n",
      "IG at  5: 0.07700985251661441\n",
      "IG at  6: 0.0024889884332655043\n",
      "IG at  7: 0.015066622054149992\n",
      "IG at  8: 0.025819023909141148\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: car.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "unacc            0.931782  0.959504  0.922454  0.945440\n",
      "acc              0.708333  0.752604  0.876157  0.729798\n",
      "vgood            0.951220  0.600000  0.983796  0.735849\n",
      "good             0.636364  0.304348  0.965278  0.411765\n",
      "macro-averaged   0.806925  0.654114  0.936921  0.705713\n",
      "\n",
      "Root entropy: 1.2057409700121753\n",
      "IG at  0: 0.09644896916961399\n",
      "IG at  1: 0.07370394692148596\n",
      "IG at  2: 0.004485716626632108\n",
      "IG at  3: 0.2196629633399082\n",
      "IG at  4: 0.030008141247605424\n",
      "IG at  5: 0.26218435655426386\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: cmc.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "No-use           0.613487  0.593005  0.666667  0.603072\n",
      "Long-term        0.395445  0.573574  0.705363  0.468137\n",
      "Short-term       0.473822  0.354207  0.639511  0.405375\n",
      "macro-averaged   0.494251  0.506929  0.670514  0.492195\n",
      "\n",
      "Root entropy: 1.5390345832497478\n",
      "IG at  0: 0.07090633894894594\n",
      "IG at  1: 0.04013859922938412\n",
      "IG at  2: 0.10173991727554088\n",
      "IG at  3: 0.009820501434384843\n",
      "IG at  4: 0.002582332379721608\n",
      "IG at  5: 0.030474214560266555\n",
      "IG at  6: 0.032511460053806784\n",
      "IG at  7: 0.015786455595620197\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: hepatitis.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "LIVE             0.923729  0.886179  0.851613  0.904564\n",
      "DIE              0.621622  0.718750  0.851613  0.666667\n",
      "macro-averaged   0.772675  0.802464  0.851613  0.785615\n",
      "\n",
      "Root entropy: 0.7346451526501956\n",
      "IG at  0: 0.03660746514280977\n",
      "IG at  1: 0.015265380561918285\n",
      "IG at  2: 0.014490701150154384\n",
      "IG at  3: 0.08645063847884216\n",
      "IG at  4: 0.08322845589007444\n",
      "IG at  5: 0.013806029835453981\n",
      "IG at  6: 0.02583616543298528\n",
      "IG at  7: 0.019946706737058673\n",
      "IG at  8: 0.03545129930993951\n",
      "IG at  9: 0.10609940973733889\n",
      "IG at 10: 0.12834719717550025\n",
      "IG at 11: 0.07683374085598982\n",
      "IG at 12: 0.08493296456638777\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: hypothyroid.csv\n",
      "\n",
      "                Precision  Recall  Accuracy  F1_Score\n",
      "hypothyroid      0.000000     0.0  0.952261  0.000000\n",
      "negative         0.952261     1.0  0.952261  0.975547\n",
      "macro-averaged   0.476130     0.5  0.952261  0.487773\n",
      "\n",
      "Root entropy: 0.27671573102958635\n",
      "IG at  0: 0.00044766932839335194\n",
      "IG at  1: 0.0009139351160850073\n",
      "IG at  2: 0.0012382074503017315\n",
      "IG at  3: 0.00014844815831743796\n",
      "IG at  4: 0.0009985293906336068\n",
      "IG at  5: 0.0013683791752741592\n",
      "IG at  6: 0.0005423006444424394\n",
      "IG at  7: 0.0004350938464638965\n",
      "IG at  8: 0.0004888757691284829\n",
      "IG at  9: 0.0008983004044028076\n",
      "IG at 10: 4.463778824304043e-05\n",
      "IG at 11: 7.8684698479492e-05\n",
      "IG at 12: 0.009353710215580346\n",
      "IG at 13: 0.004075493419623766\n",
      "IG at 14: 0.005792553705846859\n",
      "IG at 15: 0.005768288201614624\n",
      "IG at 16: 0.005744031245602799\n",
      "IG at 17: 0.002580427555574416\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: mushroom.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "p                0.994755  0.968590  0.982398  0.981498\n",
      "e                0.971468  0.995247  0.982398  0.983214\n",
      "macro-averaged   0.983112  0.981919  0.982398  0.982356\n",
      "\n",
      "Root entropy: 0.9990678968724604\n",
      "IG at  0: 0.048796701935373\n",
      "IG at  1: 0.028590232773772817\n",
      "IG at  2: 0.03604928297620391\n",
      "IG at  3: 0.19237948576121966\n",
      "IG at  4: 0.9060749773839998\n",
      "IG at  5: 0.014165027250616302\n",
      "IG at  6: 0.10088318399657026\n",
      "IG at  7: 0.23015437514804615\n",
      "IG at  8: 0.41697752341613137\n",
      "IG at  9: 0.007516772569664321\n",
      "IG at 10: 0.13481763762727572\n",
      "IG at 11: 0.2847255992184845\n",
      "IG at 12: 0.2718944733927464\n",
      "IG at 13: 0.2538451734622399\n",
      "IG at 14: 0.24141556652756657\n",
      "IG at 15: 0.0\n",
      "IG at 16: 0.0238170161209168\n",
      "IG at 17: 0.03845266924309054\n",
      "IG at 18: 0.3180215107935376\n",
      "IG at 19: 0.4807049176849154\n",
      "IG at 20: 0.2019580190668524\n",
      "IG at 21: 0.1568336046050921\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: nursery.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "recommend        0.000000  0.000000  0.999846  0.000000\n",
      "priority         0.820972  0.902954  0.903241  0.860013\n",
      "not_recom        1.000000  1.000000  1.000000  1.000000\n",
      "very_recom       0.909091  0.060976  0.976080  0.114286\n",
      "spec_prior       0.894549  0.868447  0.927006  0.881305\n",
      "macro-averaged   0.724922  0.566475  0.961235  0.571121\n",
      "\n",
      "Root entropy: 1.7164959001837934\n",
      "IG at  0: 0.07293460750309988\n",
      "IG at  1: 0.1964492804881155\n",
      "IG at  2: 0.005572591715219843\n",
      "IG at  3: 0.011886431475775838\n",
      "IG at  4: 0.019602025022871672\n",
      "IG at  5: 0.0043331270252002785\n",
      "IG at  6: 0.022232616894018342\n",
      "IG at  7: 0.9587749604699762\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Dataset: primary-tumor.csv\n",
      "\n",
      "                Precision    Recall  Accuracy  F1_Score\n",
      "A                0.812500  0.773810  0.899705  0.792683\n",
      "B                0.869565  1.000000  0.991150  0.930233\n",
      "C                0.666667  0.222222  0.976401  0.333333\n",
      "D                0.416667  0.357143  0.952802  0.384615\n",
      "E                0.344828  0.256410  0.858407  0.294118\n",
      "F                0.333333  1.000000  0.994100  0.500000\n",
      "G                0.500000  0.071429  0.958702  0.125000\n",
      "H                0.400000  0.333333  0.979351  0.363636\n",
      "J                0.500000  1.000000  0.994100  0.666667\n",
      "K                0.394737  0.535714  0.893805  0.454545\n",
      "L                0.400000  0.750000  0.935103  0.521739\n",
      "M                1.000000  0.285714  0.985251  0.444444\n",
      "N                0.529412  0.375000  0.932153  0.439024\n",
      "O                1.000000  0.500000  0.997050  0.666667\n",
      "P                1.000000  1.000000  1.000000  1.000000\n",
      "Q                0.368421  0.700000  0.955752  0.482759\n",
      "R                0.609756  0.862069  0.941003  0.714286\n",
      "S                1.000000  0.333333  0.988201  0.500000\n",
      "T                0.666667  1.000000  0.997050  0.800000\n",
      "U                1.000000  1.000000  1.000000  1.000000\n",
      "V                0.826087  0.791667  0.973451  0.808511\n",
      "macro-averaged   0.649459  0.626088  0.962073  0.582012\n",
      "\n",
      "Root entropy: 3.6437400563509668\n",
      "IG at  0: 0.1547421418870596\n",
      "IG at  1: 0.33536005150555503\n",
      "IG at  2: 0.5523526233809046\n",
      "IG at  3: 0.3791042602771899\n",
      "IG at  4: 0.21246189904816637\n",
      "IG at  5: 0.0203669388480483\n",
      "IG at  6: 0.10088123982399111\n",
      "IG at  7: 0.0678727757044233\n",
      "IG at  8: 0.22052193470670511\n",
      "IG at  9: 0.1997614363902529\n",
      "IG at 10: 0.06714460241010656\n",
      "IG at 11: 0.06025390884525317\n",
      "IG at 12: 0.29153013602249356\n",
      "IG at 13: 0.12715354518198252\n",
      "IG at 14: 0.2458886814337724\n",
      "IG at 15: 0.18425767171538476\n",
      "IG at 16: 0.17014811083887338\n",
      "\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Naive Bayes Classifier ###\n",
    "\n",
    "# All the csv files \n",
    "datasets = ['anneal.csv', \n",
    "            'breast-cancer.csv',\n",
    "            'car.csv',\n",
    "            'cmc.csv',\n",
    "            'hepatitis.csv',\n",
    "            'hypothyroid.csv',\n",
    "            'mushroom.csv',\n",
    "            'nursery.csv',\n",
    "            'primary-tumor.csv']\n",
    "\n",
    "\n",
    "# Run the NB classifier on each dataset\n",
    "# Evaluate the predictions for their accurary\n",
    "# Calculate the information gain for each of their attributes\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(\"Dataset: \" + dataset +\"\\n\") \n",
    "    \n",
    "    data, target = preprocess(dataset)\n",
    "    nb_model     = train(data, target)\n",
    "    predictions  = predict(nb_model, data, k=0.1)\n",
    "    evaluate(predictions, target)\n",
    "    info_gains   = info_gain(nb_model, [i for i in range(len(data[0]))])\n",
    "    \n",
    "    # Print the information gains calculated\n",
    "    for attr, ig in info_gains.items():\n",
    "        if attr == 'h_r': \n",
    "            print(\"\\nRoot entropy: \" + str(ig))\n",
    "        else :\n",
    "            print(\"IG at %2d\" %(attr) + \": \" + str(ig))\n",
    "    print('\\n==========================================================\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Answer:\n",
    "By the definition of information gain, it is “the expected reduction in the entropy (the amount of unpredictability) caused by knowing the values of an attribute”. \n",
    "\n",
    "If an attribute has high information gain relative to the class distribution, by knowing the value of the attribute, it reduces the amount of unpredictability for the given class, or in other words, the classifier is able to identify the class label that is generated partly from the attribute value with more confidence. Hence, attribute that have information gain increases the likelihood for the classifier to make accurate predictions. \n",
    "\n",
    "However, information gain alone is not sufficient to explain the behaviour of the classifier in the entropy context. Root entropy (the unpredictability in the class distribution) together with information gain can then largely suggest the effectiveness of the classifier. High root entropy suggests high unpredictability, that the class distribution is fairly even, no particular class label is significantly more probable than the other, predictions are unlikely to be accurate as a result. Hence, in most cases where root entropy is high, attributes with high information gain are necessary to make accurate predictions, conversely, in most cases where root entropy is low, it is not necessary to have attributes with high information gain relatively in order to make accurate predictions.\n",
    "\n",
    "One particular surprising result is that the value of accuracy stays the same even with respect to different attributes. This is observed across several datasets, namely 'breast-cancer.csv', 'hepatitis.csv', 'hypothyroid.csv', and 'mushroom.csv'. The reason being that all these datasets only have two class labels, where one true positive prediction with respect to the first class label becomes the true negative prediction with respect to the second class. Since that accuracy by definition is (true positives + true negatives)/(total predictions) and that the values of sums of true positives and true negatives are the same for both class labels, they have the same accuracy value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Answer:\n",
    "Yes, the effectiveness of the classifier is affected by the implementation of add-k smoothing. \n",
    "\n",
    "The issue of Naive Bayes classifier without any smoothing regimes is that it tends to discard the probability of a class label once an unseen attribute value is encountered with respect to the class label, thereby leading it to either overestimate or underestimate the probabilities of other class labels, making inaccurate predictions as a result. \n",
    "\n",
    "In theory, add-k smoothing could solve this issue. It adds k sets of instances to the trained model, with the amount of instances determined by the number of unique attribute values and that each instance has an unique attribute value (including the unseen attribute value). As a result, all counts are increased by k to ensure that monotonicity is maintained, and that the unseen attribute value gets a count of k as well. \n",
    "\n",
    "With the appropriate value of k, probability of a given class label that contains unseen attribute values could be calculated without overestimating or underestimating the likelihood of the unseen attribute values, which in turn increases the accuracy of the predictions. Hence, in most cases, it would increase the effectiveness of the classifier.\n",
    "\n",
    "However, the value of k has to be low enough relatively to the size of the training data so that the number of instances added will not significantly change the class distribution, otherwise bias will be introduced. Moreover, the optimum value of k is rather hard to be obtained without any testing or domain knowledge in practice. As a result, without using an appropriate value of k, add-k smoothing would not increase the effectiveness of the classifier, or in worse cases, it would decrease the effectiveness instead.\n",
    "\n",
    "In testing the effect of add-k smoothing on the classifier, with same given datasets and evaluation strategy in each test, a range of 0 (equivalent to not having a smoothing regime) to 1 was applied as the k value with total of 10 tests, starting at 0 initially then incrementally increase the k value by 0.1 until it gets to 1. The accuracy can be seen to decrease increasingly as the value of k gets larger, this observation is consistent across all datasets. One of possible explanations for this is as stated above, that the k values with range of 0 to 1 are relatively large to the size of the training data, hence artificially causes changes to the class distribution, making the predictions inaccurate as a result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
